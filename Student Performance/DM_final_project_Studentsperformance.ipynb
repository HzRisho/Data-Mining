{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRYaoclME1R4"
      },
      "source": [
        "##**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyQ1f5B4VvcE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDC3PmPrE5_P"
      },
      "source": [
        "##**Load the Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9dfShQJV7pU",
        "outputId": "90ac9d90-10f8-44b2-96ab-f5e961c720cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c60dd6fffc70>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/StudentsPerformance.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/StudentsPerformance.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPEzAkHNWAdY"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk9zHtOaEod6"
      },
      "source": [
        "Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY6F58J5ET2K"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape Of The Dataset: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NNS_QHv46g1"
      },
      "outputs": [],
      "source": [
        "#Checking Data Types and the Missing Value\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0dto0j-4sOz"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "728bK5YtcJ2_"
      },
      "outputs": [],
      "source": [
        "df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrwltLI5FEzZ"
      },
      "outputs": [],
      "source": [
        "#Detect Outliers Using Z-Score Method\n",
        "z_scores_math = np.abs((df['math score'] - df['math score'].mean()) / df['math score'].std())\n",
        "z_scores_reading = np.abs((df['reading score'] - df['reading score'].mean()) / df['reading score'].std())\n",
        "z_scores_writing = np.abs((df['writing score'] - df['writing score'].mean()) / df['writing score'].std())\n",
        "\n",
        "# Define threshold for outliers\n",
        "threshold = 3\n",
        "\n",
        "# Identify outliers\n",
        "outliers_math = df[z_scores_math > threshold]\n",
        "outliers_reading = df[z_scores_reading > threshold]\n",
        "outliers_writing = df[z_scores_writing > threshold]\n",
        "\n",
        "# Handle outliers (by removing it)\n",
        "df = df[(z_scores_math <= threshold) & (z_scores_reading <= threshold) & (z_scores_writing <= threshold)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUwHQmYE4vkx"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYSdpyn32U1r"
      },
      "outputs": [],
      "source": [
        "df['Percentage'] = round((df['reading score'] + df['writing score'] + df['math score']) / 3, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9wkWX1qGLP3"
      },
      "source": [
        "##**Data Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsAXAq8hIF7X"
      },
      "outputs": [],
      "source": [
        "gender = df['gender'].value_counts()\n",
        "fig = px.pie(values = gender.values,\n",
        "             names = gender.index,\n",
        "             hole = 0.8)\n",
        "\n",
        "fig.update_traces(textinfo = 'label+percent', textfont_size=16)\n",
        "\n",
        "fig.update_layout(\n",
        "    font = dict(size = 20, family = \"arial\"),\n",
        "    annotations = [dict(text = 'Gender', x = 0.5, y = 0.5, font_size = 20, showarrow=False)]\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8DTKmdfG5nZ"
      },
      "outputs": [],
      "source": [
        "ethnicity = df['race/ethnicity'].value_counts()\n",
        "fig = px.pie(values = ethnicity.values,\n",
        "             names = ethnicity.index,\n",
        "             hole = 0.8)\n",
        "\n",
        "fig.update_traces(textinfo = 'label+percent', textfont_size=16)\n",
        "\n",
        "fig.update_layout(\n",
        "    font = dict(size = 20, family = \"arial\"),\n",
        "    annotations = [dict(text = 'Race distribution', x = 0.5, y = 0.5, font_size = 20, showarrow=False)]\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfO--gB_IZg5"
      },
      "outputs": [],
      "source": [
        "test_preparation_course= df['test preparation course'].value_counts()\n",
        "fig = px.pie(values = test_preparation_course.values,\n",
        "             names = test_preparation_course.index,\n",
        "             hole = 0.8)\n",
        "\n",
        "fig.update_traces(textinfo = 'label+percent', textfont_size=14)\n",
        "\n",
        "fig.update_layout(\n",
        "    font = dict(size = 15, family = \"arial\"),\n",
        "    annotations = [dict(text = \"test preparation course\", x = 0.5, y = 0.5, font_size = 20, showarrow=False)]\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soRnd-7BIefS"
      },
      "outputs": [],
      "source": [
        "parental_education = df['parental level of education'].value_counts()\n",
        "fig = px.pie(values = parental_education.values,\n",
        "             names = parental_education.index,\n",
        "             hole = 0.8)\n",
        "\n",
        "fig.update_traces(textinfo = 'label+percent', textfont_size=14)\n",
        "\n",
        "fig.update_layout(\n",
        "    font = dict(size = 15, family = \"arial\"),\n",
        "    annotations = [dict(text = \"Parent's Education\", x = 0.5, y = 0.5, font_size = 20, showarrow=False)]\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwTinfNMGmgk"
      },
      "outputs": [],
      "source": [
        "#Distribution of the Scores in Math, Reading, and Writing\n",
        "# Create subplots for each score\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Histogram for Math Score\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df['math score'], color='skyblue', edgecolor='black')\n",
        "plt.title('Math Score Distribution')\n",
        "plt.xlabel('Math Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Histogram for Reading Score\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(df['reading score'], color='salmon', edgecolor='black')\n",
        "plt.title('Reading Score Distribution')\n",
        "plt.xlabel('Reading Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Histogram for Writing Score\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(df['writing score'], color='lightgreen', edgecolor='black')\n",
        "plt.title('Writing Score Distribution')\n",
        "plt.xlabel('Writing Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJOC_YfJPSc"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize = (25, 12))\n",
        "\n",
        "sns.kdeplot(data = df, x = \"Percentage\", hue = \"gender\",palette = 'inferno', cumulative = True, common_norm = False, ax = ax[0, 0])\n",
        "sns.kdeplot(data = df, x = \"Percentage\", hue = \"race/ethnicity\",palette = 'inferno',cumulative = True, common_norm = False, ax = ax[0, 1])\n",
        "sns.kdeplot(data = df, x = \"Percentage\", hue = \"parental level of education\",palette = 'inferno', cumulative = True, common_norm = False, ax = ax[1, 0])\n",
        "sns.kdeplot(data = df, x = \"Percentage\", hue = \"test preparation course\",palette = 'inferno', cumulative = True, common_norm = False, ax = ax[1, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH6n6ND8KIUC"
      },
      "source": [
        "- Females have higher percentage than males\n",
        "- Students whose parents holds a master's degree have a higher percentage than others\n",
        "- Students who completed their course have higher percentage as compared to those who didn't.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHX0RV6NLjpH"
      },
      "source": [
        "- Students whose parents never went to college seems to have the lowest percentage\n",
        "- Students whose parents have a master's degree performed the best followed by parents having a bachelor's degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8EBEOgs8aQo"
      },
      "outputs": [],
      "source": [
        "fig , ax = plt.subplots(1, 3, figsize = (15, 5))\n",
        "ax1 = sns.histplot(x = df['writing score'], hue = df['gender'] ,palette= 'plasma', ax= ax[0])\n",
        "ax1 = sns.histplot(x = df['reading score'], hue = df['gender'] ,palette= 'plasma', ax= ax[1])\n",
        "ax1 = sns.histplot(x = df['math score'],    hue = df['gender'] ,palette= 'plasma', ax= ax[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLEqGSuO_OQm"
      },
      "outputs": [],
      "source": [
        "# figure out the performance of each field for male and female.\n",
        "gender_mean_score = df.groupby('gender')[['math score', 'reading score', 'writing score']].mean().round(2).transpose()\n",
        "print(gender_mean_score)\n",
        "\n",
        "fig = go.Figure(data = [\n",
        "    go.Table(\n",
        "        header = {\n",
        "            'values': ['', '<b>Female</b>', '<b>Male</b>'],\n",
        "            'line_color' : 'navy',\n",
        "            'fill_color' : 'darkcyan',\n",
        "            'align' : 'center',\n",
        "            'font_size': 20\n",
        "         },\n",
        "        cells = {\n",
        "            'values' : [gender_mean_score.index, gender_mean_score['female'], gender_mean_score['male']],\n",
        "            'line_color' : 'navy',\n",
        "            'fill_color' : 'azure',\n",
        "            'align' : 'center',\n",
        "            'height' : 40,\n",
        "            'font_size': 20\n",
        "        }\n",
        "    )\n",
        "])\n",
        "fig.update_layout(width = 600, height = 400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_oTPiEqFOja"
      },
      "source": [
        "- Females tend to do better than males in both reading and writing\n",
        "- Males perform better in Maths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj1Mmoitcdhz"
      },
      "outputs": [],
      "source": [
        "df_corr = df.iloc[:,5:8]\n",
        "df_corr=df_corr.corr()\n",
        "df_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Q2v1j8Jk5L"
      },
      "source": [
        "- Almost all of these scores are highly correlated with each other\n",
        "- Maths score seems to be the least correlated among these, therefore we will try to predict maths score during modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6Re0ApcJ-cU"
      },
      "outputs": [],
      "source": [
        "def Grade(percentage):\n",
        "    if percentage >= 95: return \"A+\"\n",
        "    if percentage > 81 : return \"A\"\n",
        "    if percentage > 71 : return \"B\"\n",
        "    if percentage > 61 : return \"C\"\n",
        "    if percentage > 51 : return \"D\"\n",
        "    if percentage > 41 : return \"E\"\n",
        "    else: return \"F\"\n",
        "\n",
        "df[\"Grade\"] = df['Percentage'].apply(lambda x: Grade(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq1YWwDALeKI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "sns.barplot(x = 'Grade', y = 'Percentage', data= df, hue= 'gender', palette= 'crest');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoM_d1ekBQmM"
      },
      "source": [
        "##**Transformation pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL-W0EdSCPix"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ9Go0oPNyQF"
      },
      "outputs": [],
      "source": [
        "class CustomOrdinalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, grades_ordering = ['F', 'E', 'D', 'C', 'B', 'A', 'A+'],\n",
        "                 ethnicity_ordering = ['group A', 'group B', 'group C', 'group D', 'group E'],\n",
        "                 parents_education_ordering = ['high school', 'some high school', 'some college', \"associate's degree\", \"bachelor's degree\", \"master's degree\"]):\n",
        "\n",
        "        self.grades_ordering = grades_ordering\n",
        "        self.ethnicity_ordering = ethnicity_ordering\n",
        "        self.parents_education_ordering = parents_education_ordering\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X[\"Grade\"] = X['Grade'].apply(lambda x: self.grades_ordering.index(x))\n",
        "        X[\"parental level of education\"] = X['parental level of education'].apply(lambda x: self.parents_education_ordering.index(x))\n",
        "        X[\"race/ethnicity\"] = X['race/ethnicity'].apply(lambda x: self.ethnicity_ordering.index(x))\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeGj-PASdokU"
      },
      "outputs": [],
      "source": [
        "num_cols = ['reading score', 'writing score', 'Percentage']\n",
        "cat_cols = ['gender', 'lunch', 'test preparation course']\n",
        "ordinal_cols = ['Grade', 'race/ethnicity', 'parental level of education']\n",
        "\n",
        "pipeline = ColumnTransformer([\n",
        "    ('std_scaler', StandardScaler(), num_cols),\n",
        "    ('ord_encode', CustomOrdinalEncoder(), ordinal_cols),\n",
        "    ('label_encode', OneHotEncoder(), cat_cols)], remainder= 'passthrough')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6wbvpZ5gF_f"
      },
      "outputs": [],
      "source": [
        "df['Percentage'] = round((df['reading score'] + df['writing score']) / 2, 2)\n",
        "df[\"Grade\"] = df['Percentage'].apply(lambda x: Grade(x))\n",
        "x = df.drop('math score', axis = 1)\n",
        "y = df['math score']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2)\n",
        "X_train_prepared = pipeline.fit_transform(X_train)\n",
        "X_test_prepared = pipeline.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "f5Km4R6Kf7-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqKdj1Rf9Jf_"
      },
      "source": [
        "##**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QsKZ6F79okT"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE3kCG_v3llT"
      },
      "outputs": [],
      "source": [
        "#SVM\n",
        "# kernel = linear\n",
        "model_linear = SVC(kernel=\"linear\")\n",
        "model_linear.fit(X_train_prepared, y_train)\n",
        "pred_test_linear = model_linear.predict(X_test_prepared)\n",
        "pred_train_linear = model_linear.predict(X_train_prepared)\n",
        "\n",
        "# kernel = sigmoid\n",
        "model_sigmoid = SVC(kernel=\"sigmoid\")\n",
        "model_sigmoid.fit(X_train_prepared, y_train)\n",
        "pred_test_sigmoid = model_sigmoid.predict(X_test_prepared)\n",
        "pred_train_sigmoid = model_sigmoid.predict(X_train_prepared)\n",
        "\n",
        "\n",
        "\n",
        "data = {\"kernel\":pd.Series([\"linear\",\"sigmoid\"]),\"Test Accuracy\":pd.Series([accuracy_score(y_test, pred_test_linear),accuracy_score(y_test, pred_test_sigmoid)])}\n",
        "table_acc=pd.DataFrame(data)\n",
        "table_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_JT6eI-3oXw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "kernels = [\"linear\", \"sigmoid\"]\n",
        "test_accuracy = [accuracy_score(y_test, pred_test_linear), accuracy_score(y_test, pred_test_sigmoid)]\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.35\n",
        "\n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(kernels))\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(r1, test_accuracy, color=['blue', 'green'], width=bar_width, edgecolor='grey', label='Test Accuracy')\n",
        "\n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Kernel', fontweight='bold')\n",
        "plt.ylabel('Accuracy', fontweight='bold')\n",
        "plt.xticks([r for r in range(len(kernels))], kernels)\n",
        "\n",
        "# Add labels\n",
        "for i, v in enumerate(test_accuracy):\n",
        "    plt.text(i, v + 0.01, str(round(v, 2)), ha='center', va='bottom')\n",
        "\n",
        "# Add title and legend\n",
        "plt.title('Test Accuracy Comparison of SVM Models with Different Kernels')\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Random Forest**"
      ],
      "metadata": {
        "id": "WcfKcfvCeynR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optun"
      ],
      "metadata": {
        "id": "u4a5Xn_DenQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score, KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "K0L_K4M8e0ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "model = RandomForestRegressor(random_state = 42)\n",
        "model.fit(X_train_prepared, y_train)\n",
        "\n",
        "kfold = KFold(n_splits= 5)\n",
        "scores =  - cross_val_score(model, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=kfold)\n",
        "rmse_scores = np.sqrt(scores)\n",
        "\n",
        "print(f\"Mean: {rmse_scores.mean()}\", )\n",
        "print(f\"Standard deviation: {rmse_scores.std()}\")"
      ],
      "metadata": {
        "id": "sWTipSVWe35D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R5FkSAH8seyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_prepared)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared= False)\n",
        "r_square = r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "DZH2cGB-fFy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Root Mean Squared error: {round(rmse, 3)}')\n",
        "print(f'R-square: {round(r_square, 3)}')"
      ],
      "metadata": {
        "id": "vdia4xflfTg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_context('notebook', font_scale= 1.3)\n",
        "plt.figure(figsize= (10, 6))\n",
        "sns.scatterplot(x= y_test, y= y_pred, color= '#005b96')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ecCqJf8ftL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-N88WJwTA1S"
      },
      "source": [
        "##**K mean**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS7NiS6rTZWE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VADqcEB8TC4D"
      },
      "outputs": [],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "train_df = df.copy()\n",
        "train_df[\"parental level of education\"] = labelencoder.fit_transform(train_df[\"parental level of education\"])\n",
        "train_df[\"test preparation course\"] = labelencoder.fit_transform(train_df[\"test preparation course\"])\n",
        "train_df[\"lunch\"] = labelencoder.fit_transform(train_df[\"lunch\"])\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl1W7wFfT8zF"
      },
      "outputs": [],
      "source": [
        "kmeans_dis = list()\n",
        "for idx in range(2, 25):\n",
        "    kmeans = KKMeansMeans(init = \"k-means++\", n_clusters = idx, n_init = 20)\n",
        "    kmeans.fit_transform(train_df.iloc[:, 2:-1])\n",
        "    kmeans_dis.append(kmeans.inertia_)\n",
        "plt.plot(list(range(2,25)), kmeans_dis, marker = \"o\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"Summation of distance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTLCLcbAUMTw"
      },
      "source": [
        "We choose 8 as elbow point, and then classify all data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPs402_nUNKu"
      },
      "outputs": [],
      "source": [
        "#KMeans\n",
        "kmeans = KMeans(init = \"k-means++\", n_clusters = 8)\n",
        "kmeans.fit_transform(train_df.iloc[:, 2:-1])\n",
        "kmeans_label = kmeans.labels_\n",
        "df[\"clusters\"] = kmeans_label\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACLZFCYvUY27"
      },
      "outputs": [],
      "source": [
        "class_df = df.groupby(\"clusters\")[df.columns[5:8]].mean()\n",
        "class_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xTNcmExWF7m"
      },
      "outputs": [],
      "source": [
        "ind = np.arange(8)\n",
        "width = 0.35\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(ind - width/2, class_df['math score'], width, label='Math')\n",
        "rects2 = ax.bar(ind, class_df['reading score'], width, label='Reading')\n",
        "rects3 = ax.bar(ind + width/2, class_df['writing score'], width, label='Writing')\n",
        "\n",
        "ax.set_xlabel('Classiffication')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_xticks(ind)\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqC7YCSnXgAo"
      },
      "source": [
        "it's obviously that all subject of cluster has the same trend, so we choose the average of all sbjects to rank the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EITeq50RXgvR"
      },
      "outputs": [],
      "source": [
        "class_df[\"total_ave_score\"] = (class_df['math score'] + class_df['reading score'] + class_df['writing score'])/3\n",
        "rank = class_df[\"total_ave_score\"].sort_values(ascending = False)\n",
        "rank.index\n",
        "rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZDwWahwaK38"
      },
      "source": [
        "For top5 rank, the average score all passed, Rank0 is the best cluster, Rank1 is second one and so on.\n",
        "\n",
        "From now on, we can find out the correlation between the performance of students and features. Let's plot pie chart to see whether parents education level can affect the performance or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB5GhSCEaMVK"
      },
      "outputs": [],
      "source": [
        "def plot_pie_chart(column):\n",
        "    fig, ax = plt.subplots(figsize=(20,16))\n",
        "    color = [\"orange\",\"lightblue\",\"green\",\"yellow\",\"red\",\"pink\",\"brown\",\"gray\"]\n",
        "    for idx in range(8):\n",
        "        plt.subplot(3, 3, idx+1)\n",
        "        num = \"class\"+ str(idx)\n",
        "        num = df[df[\"clusters\"]==rank.index[idx]]\n",
        "        percentage_of_parent_edu = num[column].value_counts()\n",
        "        percentage_of_parent_edu.sort_index()\n",
        "        label = percentage_of_parent_edu.index\n",
        "        value = percentage_of_parent_edu.values\n",
        "        plt.pie(value, labels = label, autopct = \"%1.1f%%\",\n",
        "                startangle=90, radius = 4, colors = color[:len(label)])\n",
        "        plt.axis(\"equal\")\n",
        "        plt.title(\"Rank \"+str(idx))\n",
        "    plt.show()\n",
        "plot_pie_chart(\"parental level of education\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pxgkK8DaWsu"
      },
      "source": [
        "Let's define the high degree of education. Parents having bachelor or master degree are high-level educated. So we focus on these two terms.\n",
        "\n",
        "As pie chart were shown above, we can easily understand the ratio of high-degree education. For the rank0, its ratio is around 32%. In addition, there are no differences between rank1 to rank3, and the ratio are around 15~17%. Finally, the ratio is only 8% in rank7.\n",
        "\n",
        "We calculated the average score of each rank before, so we can say that parent's education affect the score but not obviously, because there are still 70%~80% parents without high education degree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjLqZ0e4a8gE"
      },
      "outputs": [],
      "source": [
        "def plot_bar_chart(column):\n",
        "    fig, ax = plt.subplots(figsize=(8,6))\n",
        "    index_dict = dict()\n",
        "    width = 0.35\n",
        "    ind = np.arange(8)\n",
        "    for idx in range(8):\n",
        "        num = \"class\"+ str(idx)\n",
        "        num = df[df[\"clusters\"]==rank.index[idx]]\n",
        "        percentage_of_column = num[column].value_counts()\n",
        "        percentage_of_column = percentage_of_column.sort_index()\n",
        "        for key in percentage_of_column.index:\n",
        "            if key not in index_dict.keys():\n",
        "                index_dict[key] = []\n",
        "                index_dict[key].append(percentage_of_column[key]) #/percentage_of_column.values.sum())\n",
        "            else:\n",
        "                index_dict[key].append(percentage_of_column[key]) #/percentage_of_column.values.sum())\n",
        "\n",
        "    percentage_of_column = df[df[\"clusters\"]==rank.index[4]][column].value_counts().sort_index()\n",
        "    for i in range(len(percentage_of_column.index)):\n",
        "        rects = ax.bar(ind - width/(i+1),\n",
        "                       index_dict[percentage_of_column.index[i]],\n",
        "                       width, label=percentage_of_column.index[i])\n",
        "\n",
        "    ax.set_xlabel('Rank')\n",
        "    ax.set_ylabel('# of students')\n",
        "    ax.set_title(\"Percentage of \" + column)\n",
        "    ax.set_xticks(ind)\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_bar_chart(\"test preparation course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAoo4rdgbSgv"
      },
      "source": [
        "Over 50% of students in rank0 completed the test preparation course, and normally there is about 70~80% students in rank7 hadn't finished course. It is say that preparation course can help students get better score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V_CfrQ3bTDy"
      },
      "outputs": [],
      "source": [
        "plot_bar_chart(\"lunch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZON9A7bdjA"
      },
      "source": [
        "Students who had lunch before test got better score. That is, it's hard to get good performance without eating.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_qxURepbXEu"
      },
      "outputs": [],
      "source": [
        "plot_bar_chart(\"gender\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rc8WCZSbaJO"
      },
      "source": [
        "It's hard to say that the male is better than female."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_6HdhQcbiYc"
      },
      "source": [
        "There are few conclusions below:\n",
        "\n",
        "Parents' education level may affect the performance of students, but not the important one.\n",
        "Finishing preparation course is benefitial.\n",
        "Having lunch is important to students, and it is also the most significant one.\n",
        "Gender has no correlation with the score.\n",
        "In summary, if students want to have good performance, they should have enough nutrient and make effort to prepare the test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDO9eizVeeuN"
      },
      "source": [
        "How effective is the test preparation course?\n",
        "Its very effective because the students who took the course got higher test scores on all the tests unlike to those who didnt took the course\n",
        "\n",
        "Which major factors contribute to test outcomes?\n",
        "The highest factor that contribute to test outcomes is the test preparation course and followed by the lunch and lastly is the gender, though race/ethnicity has quite influence on the test scores, its very little.\n",
        "\n",
        "What would be the best way to improve student scores on each test?\n",
        "To take the test preparation course. Even theres other factors that have a major effect on the test outcomes, its not a choice that students could change, examples: lunch, gender and race/ethnicity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Test Accuracy**"
      ],
      "metadata": {
        "id": "H9qJ4rfGlilK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # KMeans\n",
        "# y_pred_1 = kmeans.predict(X_test)\n",
        "# accuracy_1 = accuracy_score(y_test, y_pred_1)\n",
        "\n",
        "\n",
        "\n",
        "# # Plot the metrics for Model 1\n",
        "# metrics = ['Accuracy']\n",
        "# values = [accuracy_1]\n",
        "\n",
        "# plt.bar(metrics, values)\n",
        "# plt.xlabel('Metrics')\n",
        "# plt.ylabel('Values')\n",
        "# plt.title('Model 1 - Kmean')\n",
        "# plt.show()\n",
        "\n",
        "# Model 2: Decision Tree Classifier\n",
        "y_pred_2 = model.predict(X_test_prepared)\n",
        "accuracy_2 = accuracy_score(y_test, y_pred_2)\n",
        "\n",
        "\n",
        "# Plot the metrics for Model 2\n",
        "metrics = ['Accuracy']\n",
        "values = [accuracy_2]\n",
        "\n",
        "plt.bar(metrics, values)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Model 2 - RandomForestRegressor')\n",
        "plt.show()\n",
        "\n",
        "# Model 3: Decision Tree Classifier\n",
        "y_pred_2 = RandomForestRegressor.predict(X_test)\n",
        "accuracy_2 = accuracy_score(y_test, y_pred_2)\n",
        "\n",
        "\n",
        "# Plot the metrics for Model 3\n",
        "metrics = ['Accuracy']\n",
        "values = [accuracy_2]\n",
        "\n",
        "plt.bar(metrics, values)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Model 3 - ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ptCthTmfj9Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have the `table_acc` DataFrame for SVM test accuracies\n",
        "\n",
        "# Extract data\n",
        "svm_kernels = table_acc[\"kernel\"].tolist()\n",
        "svm_accuracy = table_acc[\"Test Accuracy\"].tolist()\n",
        "mean_rf_accuracy = rmse*100  # Replace with your calculated mean accuracy (or RMSE) for Random Forest\n",
        "std_dev_rf_accuracy = rmse_scores.std()  # Replace with your calculated standard deviation\n",
        "\n",
        "# Create the chart\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(svm_kernels, svm_accuracy, color=['blue', 'green'], label='SVM')  # Adjust colors and label\n",
        "plt.axhline(y=mean_rf_accuracy, color='red', linestyle='--', label='Random Forest (Mean)')  # Adjust color and label\n",
        "plt.errorbar(x=['Random Forest'], y=[mean_rf_accuracy], yerr=[std_dev_rf_accuracy], fmt='none', ecolor='red', capsize=7, label='Random Forest (Std. Dev.)')  # Adjust label\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Test Accuracy (or RMSE)\")  # Adjust label based on metric\n",
        "plt.title(\"Comparison of Test Accuracy/RMSE (Single Chart)\")\n",
        "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xQ9Mbi8cxSPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VscnL_ZAV3KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aHMsc0kayWp4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}